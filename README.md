# This the project where i have solved Pyspark real-time Industry Level Scenarios-

# Scenarios
1) S-1: Having a datalake in which the the data will get ingested frequently, i had to process the file as soon as they arrive.
2) S-2: Same as (S-1) but here the schemna of the file changed, and i need to  handle that scenario
3) S-3: Implementing a solution to handle SLDs(Slowly Changing Data) for both intial and incremental run.
4) S-4: Created a Class for Window function in Python, so whenever needed it can be used in any dataframe.
5) S-5: Created a conditional column based on the the Units-Sold column.
6) S-6: Created a parameterized Notebook that takes input from a user and use it in filter criteria, and puts out multiple outputs.
7) S-7: Figured out the skewness of data buy partioning number of records and ranking them accordingly.
8) S-8: Error handling Scenarios in Pyspark.
9) S-9: Incrementaly Streamed Data from a Delta Table and Handled updates aswell.
10) S-10: Handled Vectors( Deletion)
